{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "sn.set(rc={'figure.figsize':(9,9)})\n",
    "sn.set(font_scale=1.4)\n",
    "\n",
    "# make results reproducible\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "!pip install -q tensorflow-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "\n",
    "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
    "\n",
    "Previously in history there were allot of manual rule based approaches to various NLP tasks, however modern problems are outperformed using machine learning specifically deep neural networks. These approaches learn natural language rules through analysis of large corpora (set of documents, with possibly human or computer annotations) of typical real-world examples. Deep neural network architectures can achieve state-of-the-art results in many natural language tasks, such as language modeling, parsing, and many others. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering).\n",
    "\n",
    "# Word Embeddings\n",
    "\n",
    "Our existing models so far process numerical based data, however language consists of words so we need a way to convert or encode these (or sets of words, i.e. sentences or documents) into a representation that we can feed into our model. Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension.\n",
    "\n",
    "Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.\n",
    "\n",
    "Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis.\n",
    "\n",
    "---\n",
    "\n",
    "A naive approach to create a word embedding may be to attempt to take the possible seen vocabularly in the problem domain, and encode each word a \"one-hot\"encoded vector across our vocabulary. We could then concatenate the one-hot vectors for each word in our input. In practice this does not work as there are an estimated 13 million words in the English language, leading to a very large embedding space, which is mostly sparse zero and the vectors do not provide any notion of similarity between them. Instead we wish to find a smaller subspace that encodes the relationships between words.\n",
    "\n",
    "Previous approaches in the past have bulit up a word co-occurence matrix and performed dimensionality reduction (Singular Value Decomposition) on it, to produce a dense vector for each word.\n",
    "\n",
    "One key insight was [that](https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis) \"a word is characterized by the company it keeps\", which lead to approaches taking into account the surrounding words for the word that we are trying to learn the word embedding for. Modern approaches use neural network models, or more recently simple [log-bilinear models](https://nlp.stanford.edu/pubs/glove.pdf) to map words to a dense vector (word embedding). These models are trained by learning a representation of the word that encodes words in their context (i.e. the set of words in which they normally appear nearby to in text samples). These are often pre-trained (on large corpuses of data), and a selection these are:\n",
    "* **Word2vec**: [2013](https://arxiv.org/abs/1301.3781), two dense layer neural network. Comes with two variations of training objective, continous bag-of-words (CBOW) best used for small datasets and Skip-Gram best for large datasets. Trained on 30 billion words on English Google News Corpus.\n",
    "* **Glove**: [2014](https://nlp.stanford.edu/projects/glove/), (Global Vectors) is  essentially a log-bilinear model with a weighted least-squares objective. Few variants trained on from 6 billion words (Wikipedia 2014 + gigaword5), to crawl 840 billion words trained model.\n",
    "* **FastText**: [2016](https://arxiv.org/abs/1607.04606),  based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Few variants trained on from 16 billion words (Wikipedia 2017 + UMBC + news dataset) to crawl 600 billion words trained model.\n",
    "\n",
    "It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn. In practice it is best to use a pre-trained model with the highest dimension available (if not fine tuning the parameters), and potentially a smaller dimension if fine tuning the parameters to our dataset. Another way to think of an embedding is as \"lookup table\". After these weights have been learned, we can encode each word by looking up the dense vector it corresponds to in the table. To create sentence embeddings or document embeddings from word embedding models it is standard practice to compute mean of the individual word embeddings for that set, to generate a embedding feature for our model.\n",
    "\n",
    "As text embeddings depend on the distributional hypothesis (the surrounding words provide context) more recent approaches use **sentence embeddings** instead, such as:\n",
    "* **ELMO**: [2018](https://arxiv.org/abs/1802.05365) (Embeddings from Language Models) uses character-based word representations and bidirectional LSTMs. Trained on 30 million sentences.\n",
    "* **NNLM** (Recommended to use in practice) : [2018](https://tfhub.dev/google/nnlm-en-dim128/2) (Neural-Net Language Models) based on NNLM with three hidden layers. Token based text embedding trained on English Google News 200 billion corpus.\n",
    "\n",
    "## Keras Embedding layer\n",
    "\n",
    "Keras has a `tf.keras.layers.Embedding` layer, which makes it easy to use word embeddings. This layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter we can experiment with to see what works well for our problem, much in the same way we would experiment with the number of neurons in a Dense layer, however it is usually set for pre-trained Embedding layers. If we created an Embedding layer, the weights for the embedding layer are randomly initilized (just like our other layers). Either we use a pre-trained model, or they are gradually adjusted via backpropagation during training. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem our model is trained on).\n",
    "\n",
    "[[1](https://www.tensorflow.org/tutorials/text/word_embeddings)]\n",
    "\n",
    "If we pass an integer to the embedding layer, the result replaces each integer with the vector from the embedding table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01316785,  0.02323476, -0.01350605, -0.0314668 ,  0.04560835],\n",
       "       [ 0.04549145,  0.02358145,  0.00343136, -0.0443067 , -0.04387503],\n",
       "       [-0.00826237, -0.0350232 ,  0.02040035,  0.03441587,  0.01151514]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "embedding_layer = layers.Embedding(1000, 5)\n",
    "result = embedding_layer(tf.constant([1,2,3]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally the `tf.keras.layers.Embedding` layer takes the following arguments:\n",
    "* `input_dim`: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "* `output_dim`: int >= 0. Dimension of the dense embedding.\n",
    "\n",
    "And optional keyword arguments:\n",
    "* `embeddings_initializer`: Initializer for the embeddings matrix.\n",
    "* `embeddings_regularizer`: Regularizer function applied to the embeddings matrix.\n",
    "* `embeddings_constraint`: Constraint function applied to the embeddings matrix.\n",
    "* `mask_zero`: Whether or not the input value 0 is a special \"padding\" value that should be masked out.\n",
    "* `input_length`: Length of input sequences, when it is constant.\n",
    "\n",
    "\n",
    "\n",
    "For text or sequence problems, the Embedding layer takes a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers. It can embed sequences of variable lengths. We could feed into the embedding layer above batches with shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15).\n",
    "\n",
    "The returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a `(2, 3)` input batch and the output is `(2, 3, N)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[0,1,2],[3,4,5]]))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we often use general pre-trained word embeddings for NLP problems, due to the large number of words required to train a good word embedding. We can load one of these from tensorflow hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\",\n",
    "                           input_shape=[], dtype=tf.string, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
       "array([[-6.09472729e-02,  1.32798254e-02, -3.42817493e-02,\n",
       "         4.26401682e-02, -1.43975914e-01,  1.62000328e-01,\n",
       "        -2.93821730e-02, -2.00032350e-02, -7.07213208e-02,\n",
       "         1.63512006e-02,  9.08970013e-02, -4.92882654e-02,\n",
       "         3.67779247e-02, -3.68761532e-02,  1.69035912e-01,\n",
       "        -1.26325879e-02, -1.02184914e-01, -4.70518619e-02,\n",
       "        -2.19973363e-02, -8.74752700e-02, -5.23869321e-02,\n",
       "         3.40459906e-02,  3.22821885e-02, -3.63609865e-02,\n",
       "         1.39977902e-01,  3.29261534e-02,  3.83867393e-03,\n",
       "        -5.63085563e-02, -5.25179058e-02, -1.48635376e-02,\n",
       "         6.00850163e-03, -1.24928812e-02,  2.53655901e-03,\n",
       "         1.05012886e-01, -7.33386502e-02, -1.16906567e-02,\n",
       "        -5.32731973e-02, -8.08588266e-02,  1.42390028e-01,\n",
       "         1.20618619e-01,  6.00544587e-02,  8.53905752e-02,\n",
       "         2.86061447e-02, -6.75920993e-02, -1.22930340e-01,\n",
       "        -1.01479821e-01, -9.18542147e-02,  1.24494404e-01,\n",
       "         4.55740206e-02,  8.00369531e-02,  1.45095766e-01,\n",
       "        -1.78097233e-01,  6.97804764e-02,  5.21217063e-02,\n",
       "         7.58697540e-02, -1.71922833e-01, -1.00327246e-01,\n",
       "        -5.04648648e-02, -4.20529656e-02, -1.04125533e-02,\n",
       "        -7.06154481e-02,  9.80460923e-03, -5.18575683e-02,\n",
       "        -2.04464793e-01,  3.13249752e-02, -2.47882064e-02,\n",
       "         1.33866802e-01, -9.04462263e-02,  3.44487429e-02,\n",
       "        -8.14231113e-03,  1.04663625e-01, -1.39314290e-02,\n",
       "         1.75579235e-01, -6.72068074e-02,  7.43176881e-03,\n",
       "        -5.39280735e-02,  5.76728806e-02, -1.86116211e-02,\n",
       "        -8.02072212e-02,  3.12540308e-02,  1.35823786e-01,\n",
       "         1.76271200e-02, -9.78277903e-03,  9.97072905e-02,\n",
       "        -8.90742615e-03, -5.98569028e-02, -3.46899554e-02,\n",
       "        -1.26235291e-01, -4.38800715e-02,  7.85296410e-02,\n",
       "         1.10227905e-01,  6.01701513e-02,  2.14590281e-01,\n",
       "        -1.99781321e-02,  1.14269584e-01,  3.13391648e-02,\n",
       "         1.71466589e-01, -1.68554574e-01,  5.74404038e-02,\n",
       "        -3.29839997e-02,  2.33105853e-01, -3.86497825e-02,\n",
       "        -9.93929505e-02,  4.82873954e-02, -6.69153929e-02,\n",
       "        -7.37370253e-02, -7.58315474e-02,  1.10931896e-01,\n",
       "        -4.60815504e-02, -3.82568538e-02,  1.12563632e-01,\n",
       "        -2.13174641e-01, -1.01826914e-01,  1.56046413e-02,\n",
       "        -6.05936386e-02,  1.10277012e-01,  1.08054795e-04,\n",
       "         5.81454858e-02, -1.25620797e-01, -4.68564890e-02,\n",
       "        -2.63599120e-02, -5.75047955e-02,  9.78245139e-02,\n",
       "        -3.40318047e-02, -9.29096639e-02, -1.12283126e-01,\n",
       "         1.33225024e-01,  6.25124276e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer([\"dog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a sentence embedding layer, we can also turn sentences into embeddings easily too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
       "array([[ 2.82348961e-01,  5.27088307e-02, -5.94306886e-02,\n",
       "         1.92900240e-01, -1.83743060e-01,  1.46496981e-01,\n",
       "         9.77013707e-02, -1.07290536e-01, -2.57747229e-02,\n",
       "         4.41485010e-02,  1.70615181e-01, -1.00377649e-01,\n",
       "         9.42871347e-03,  3.02134082e-02,  1.34187490e-01,\n",
       "         5.60502484e-02, -1.07281059e-01, -3.25787924e-02,\n",
       "        -8.48783329e-02,  4.67057377e-01, -6.27068579e-02,\n",
       "         1.32775724e-01,  4.04574051e-02,  1.63751058e-02,\n",
       "         1.32634431e-01,  2.57065725e-02, -7.29926378e-02,\n",
       "         7.19335005e-02,  1.11830607e-01, -2.11250614e-02,\n",
       "         3.62873152e-02,  4.42225672e-02, -4.65986840e-02,\n",
       "         5.73276244e-02, -7.11966679e-02, -5.71498871e-02,\n",
       "        -1.18366979e-01, -1.10630527e-01, -3.84329781e-02,\n",
       "        -4.61224578e-02,  6.81087002e-02,  1.19720802e-01,\n",
       "        -1.97505840e-04, -2.34259702e-02, -2.73194835e-02,\n",
       "        -1.29417166e-01,  4.85802516e-02,  1.24888368e-01,\n",
       "        -1.13985166e-02,  1.63538218e-01,  1.82041004e-01,\n",
       "        -2.73124009e-01,  7.61881173e-02,  3.62561382e-02,\n",
       "        -9.07839742e-03, -7.42314607e-02, -4.46822904e-02,\n",
       "        -6.78729489e-02,  5.76627217e-02, -3.15912738e-02,\n",
       "        -1.17660061e-01, -3.68984975e-02, -3.97797413e-02,\n",
       "        -1.63061202e-01,  7.28102028e-02, -3.51337083e-02,\n",
       "         1.03988186e-01,  1.54522974e-02,  1.56743541e-01,\n",
       "         8.83675180e-03,  3.95835862e-02, -8.39116648e-02,\n",
       "         1.00936480e-01, -3.92631665e-02,  4.82121222e-02,\n",
       "        -2.65546851e-02,  7.47473240e-02,  4.10351530e-02,\n",
       "        -6.66156113e-02,  1.10122040e-01,  1.94364473e-01,\n",
       "         3.67324129e-02,  1.70169640e-02,  7.62910396e-03,\n",
       "        -2.64335647e-02, -1.17006628e-02, -8.83294716e-02,\n",
       "        -1.05964430e-01, -5.19166514e-02,  1.74293190e-01,\n",
       "         9.80800670e-03, -6.01496606e-04,  1.75404951e-01,\n",
       "        -5.79428039e-02,  1.68295816e-01, -5.64487390e-02,\n",
       "         4.00017016e-02, -2.12970585e-01,  2.48986468e-01,\n",
       "         3.09524741e-02,  6.76777661e-02, -3.98983136e-02,\n",
       "        -6.52715415e-02,  2.99040470e-02, -1.01506852e-01,\n",
       "        -2.02945936e-02, -1.28701448e-01, -2.59261206e-03,\n",
       "        -3.10906377e-02, -4.09740321e-02,  7.68612474e-02,\n",
       "        -1.95562154e-01, -1.98896676e-02,  5.17561436e-02,\n",
       "        -7.45286644e-02,  4.74620089e-02,  5.78474738e-02,\n",
       "         1.01901762e-01, -9.06884149e-02, -1.03578165e-01,\n",
       "        -6.73857033e-02, -8.20854008e-02,  7.67332837e-02,\n",
       "        -6.33245707e-02, -6.62377328e-02, -2.54634798e-01,\n",
       "         9.63003263e-02, -8.23300332e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer([\"The dog played with the cat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to understand and use Embeddings\n",
    "\n",
    "We can treat these text embeddings as vectors, where the Euclidean distance (or cosine similarity) between text vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding text. Sometimes nearest neighbours on a sample text can reveal similar texts. I.e. if we compute the word embedding for frog, we would commonly see other close word embeddings such as frog, frogs, toad, litoria, lizard etc.\n",
    "\n",
    "The embeddings also have linear substructures to them, enabling vector differences, i.e. let $E(w)$ be the embedding for word $w$, then we will see $E(woman)- E(man) + E(king) \\approx E(queen)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with text data\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking up a string into tokens. Commonly, these tokens are words, numbers, or punctuation.\n",
    "\n",
    "The main interfaces are `Tokenizer` and `TokenizerWithOffsets` which each have a single method `tokenize` and `tokenize_with_offsets` respectively. There are multiple tokenizers available now. Each of these implement `TokenizerWithOffsets` (which extends `Tokenizer`) which includes an option for getting byte offsets into the original string. This allows the caller to know the bytes in the original string the token was created from.\n",
    "\n",
    "All of the tokenizers return *RaggedTensors* (TensorFlow equivalent of nested variable-length lists) with the inner-most dimension of tokens mapping to the original individual strings.\n",
    "\n",
    "### WhitespaceTokenizer\n",
    "This is a basic tokenizer that splits `UTF-8` strings on International Components for Unicode (ICU) defined whitespace characters (eg. space, tab, new line).\n",
    "\n",
    "\n",
    "Most tensorflow operations expect strings to be in `UTF-8` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/samholt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/samholt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'The', b'cat', b'jumped', b'up', b'and', b'suprised', b'the', b'dog!']]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_text as text\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['The cat jumped up and suprised the dog!'])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnicodeScriptTokenizer\n",
    "This tokenizer splits UTF-8 strings based on Unicode ICU script boundaries.\n",
    "\n",
    "In practice, this is similar to the `WhitespaceTokenizer` with the most apparent difference being that it will split punctuation (USCRIPT_COMMON) from language texts (eg. USCRIPT_LATIN, USCRIPT_CYRILLIC, etc) while also separating language texts from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'The', b'cat', b'jumped', b'up', b'and', b'suprised', b'the', b'dog', b'!']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "tokens = tokenizer.tokenize(['The cat jumped up and suprised the dog!'])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lzuqb8xkG7Cp"
   },
   "source": [
    "### Unicode split\n",
    "\n",
    "When tokenizing languages without whitespace to segment words, it is common to just split by character, which can be accomplished using the unicode_split operation found in core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCKTl2mzdZIo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'\\xe4\\xbb\\x85', b'\\xe4\\xbb\\x8a', b'\\xe5\\xb9\\xb4', b'\\xe5\\x89\\x8d']]\n"
     ]
    }
   ],
   "source": [
    "tokens = tf.strings.unicode_split([u\"仅今年前\".encode('UTF-8')], 'UTF-8')\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yGs1mWkeOD8"
   },
   "source": [
    "### Offsets\n",
    "\n",
    "When tokenizing strings, it is often desired to know where in the original string the token originated from. For this reason, each tokenizer which implements `TokenizerWithOffsets` has a *tokenize_with_offsets* method that will return the byte offsets along with the tokens. The offset_starts lists the bytes in the original string each token starts at, and the offset_limits lists the bytes where each token ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8yzUormbiQwz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'The', b'cat', b'jumped', b'up', b'and', b'suprised', b'the', b'dog', b'!']]\n",
      "[[0, 4, 8, 15, 18, 22, 31, 35, 38]]\n",
      "[[3, 7, 14, 17, 21, 30, 34, 38, 39]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "(tokens, offset_starts, offset_limits) = tokenizer.tokenize_with_offsets(\n",
    "    ['The cat jumped up and suprised the dog!'])\n",
    "print(tokens.to_list())\n",
    "print(offset_starts.to_list())\n",
    "print(offset_limits.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iO-tRdH72Vl9"
   },
   "source": [
    "### Working with TensorFlow Datasets\n",
    "\n",
    "Tokenizers work as expected with the `tf.data.Dataset` object. E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MCqcHXg3gtp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'The', b'dog', b'was', b'pretty', b'chilled']]\n",
      "[[b'He', b'was', b'a', b'cool', b'cat!']]\n"
     ]
    }
   ],
   "source": [
    "docs = tf.data.Dataset.from_tensor_slices(\n",
    "    [['The dog was pretty chilled'], [\"He was a cool cat!\"]])\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
    "iterator = iter(tokenized_docs)\n",
    "print(next(iterator).to_list())\n",
    "print(next(iterator).to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8zrnuXTc3ziR"
   },
   "source": [
    "### N-grams & Sliding Window\n",
    "\n",
    "N-grams are sequential words given a sliding window size of $n$. When combining the tokens, there are three reduction mechanisms supported. For text, you would want to use `Reduction.STRING_JOIN` which appends the strings to each other. The default separator character is a space, but this can be changed with the string_separater argument.\n",
    "\n",
    "The other two reduction methods are most often used with numerical values, and these are `Reduction.SUM` and `Reduction.MEAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "czIf9HcoIquB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:  [[b'Dog', b'befriended', b'the', b'cat']]\n",
      "Bi-gram:  [[b'Dog befriended', b'befriended the', b'the cat']]\n",
      "Tri-gram:  [[b'Dog befriended the', b'befriended the cat']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Dog befriended the cat'])\n",
    "\n",
    "# Ngrams, in this case bi-gram (n = 2), tri-gram (n = 3)\n",
    "unigrams = text.ngrams(tokens, 1, reduction_type=text.Reduction.STRING_JOIN)\n",
    "print('Uni-gram: ', unigrams.to_list())\n",
    "bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)\n",
    "print('Bi-gram: ', bigrams.to_list())\n",
    "trigrams = text.ngrams(tokens, 3, reduction_type=text.Reduction.STRING_JOIN)\n",
    "print('Tri-gram: ', trigrams.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Text Classification Example\n",
    "\n",
    "In this example we'll use three different English translations of the same work, Homer's Illiad, and train a model to identify the translator given a single line of text (Classify the text). This is a fully worked example where we load the dataset from text files, process them, build our own word embedding and train a simple model for classification of the text.\n",
    "\n",
    "## Loading text data in TensorFlow\n",
    "\n",
    "Let us look at an example of how to use `tf.data.TextLineDataset` to load examples from text files. `TextLineDataset` is designed to create a dataset from a text file, in which each example is a line of text from the original file. This is potentially useful for any text data that is primarily line-based (for example, poetry or error logs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the files locally\n",
    "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "for name in FILE_NAMES:\n",
    "    text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n",
    "\n",
    "parent_dir = os.path.dirname(text_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q3sDy6nuXoNp"
   },
   "source": [
    "## Load the text into TensorFlow datasets\n",
    "\n",
    "Iterate through the files, loading each one into its own dataset.\n",
    "\n",
    "Each example needs to be individually labeled, so use `tf.data.Dataset.map` to apply a labeller function to each one. This will iterate over every example in the dataset, returning (`example, label`) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0BjCOpOh7Ch"
   },
   "outputs": [],
   "source": [
    "def labeler(example, index):\n",
    "    return example, tf.cast(index, tf.int64)\n",
    "\n",
    "\n",
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "    lines_dataset = tf.data.TextLineDataset(\n",
    "        os.path.join(parent_dir, file_name))\n",
    "    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "    labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M8PHK5J_cXE5"
   },
   "source": [
    "Combine these labeled datasets into a single dataset, and shuffle it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qd544E-Sh63L"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000\n",
    "\n",
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_dataset in labeled_data_sets[1:]:\n",
    "    all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n",
    "\n",
    "all_labeled_data = all_labeled_data.shuffle(\n",
    "    BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r4JEHrJXeG5k"
   },
   "source": [
    "We can use `tf.data.Dataset.take` and `print` to see what the `(example, label)` pairs look like. The `numpy` property shows each Tensor's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gywKlN0xh6u5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1 :  tf.Tensor(b'extremity of the danger. Agamemnon proposes to make their escape by', shape=(), dtype=string)\n",
      "Label 1 :  tf.Tensor(b\"Are driv'n; among them Ajax spreads dismay,\", shape=(), dtype=string)\n",
      "Label 0 :  tf.Tensor(b'To gather it from all the Greeks again.', shape=(), dtype=string)\n",
      "Label 0 :  tf.Tensor(b'Nought trusted he, but with an iron mace', shape=(), dtype=string)\n",
      "Label 0 :  tf.Tensor(b'By Priameian Hector, fierce as Mars,', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example, label in all_labeled_data.take(5):\n",
    "    print('Label {} : '.format(label), example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rrpU2_sfDh0"
   },
   "source": [
    "### Build a vocabulary\n",
    "\n",
    "First, build a vocabulary by tokenizing the text into a collection of individual unique words. There are a few ways to do this in both TensorFlow and Python. Here we will:\n",
    "\n",
    "1. Iterate over each example's `numpy` value.\n",
    "2. Use `tfds.features.text.Tokenizer` to split it into tokens.\n",
    "3. Collect these tokens into a Python set, to remove duplicates.\n",
    "4. Get the size of the vocabulary for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YkHtbGnDh6mg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  17178\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_tensor, _ in all_labeled_data:\n",
    "    some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
    "    vocabulary_set.update(some_tokens)\n",
    "\n",
    "vocab_size = len(vocabulary_set)\n",
    "print('Vocab size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0W35VJqAh9zs"
   },
   "source": [
    "### Encode the examples\n",
    "\n",
    "Create an encoder by passing the `vocabulary_set` to `tfds.features.text.TokenTextEncoder`. The encoder's `encode` method takes in a string of text and returns a list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkxJIVAth6j0"
   },
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6S5Qyabi-vo"
   },
   "source": [
    "For example this takes an input like the one below and processes it to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgxPZaxUuTbk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Example :  b'extremity of the danger. Agamemnon proposes to make their escape by'\n",
      "Encoded Example:  [5057, 10197, 6221, 164, 2273, 841, 9758, 4349, 15865, 11355, 10608]\n"
     ]
    }
   ],
   "source": [
    "example_text = next(iter(all_labeled_data))[0].numpy()\n",
    "encoded_example = encoder.encode(example_text)\n",
    "print('Raw Example : ', example_text)\n",
    "print('Encoded Example: ', encoded_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we can also decode our encoding, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Example:  extremity of the danger Agamemnon proposes to make their escape by\n"
     ]
    }
   ],
   "source": [
    "print('Decoded Example: ', encoder.decode(encoded_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9qHM0v8k_Mg"
   },
   "source": [
    "Now we can run the encoder on the dataset by wrapping it in `tf.py_function` and  passing that to the dataset's `map` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HcIQ7LOTh6eT"
   },
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "    encoded_text = encoder.encode(text_tensor.numpy())\n",
    "    return encoded_text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eES_Z1ia-Om-"
   },
   "source": [
    "We want to use `Dataset.map` to apply this function to each element of the dataset.  `Dataset.map` runs in graph mode.\n",
    "\n",
    "* Graph tensors do not have a value. \n",
    "* In graph mode we can only use TensorFlow Ops and functions. \n",
    "\n",
    "So we can't `.map` this function directly: We need to wrap it in a `tf.py_function`. The `tf.py_function` will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KmQVsAgJ-RM0"
   },
   "outputs": [],
   "source": [
    "def encode_map_fn(text, label):\n",
    "    # py_func doesn't set the shape of the returned tensors.\n",
    "    encoded_text, label = tf.py_function(encode,\n",
    "                                         inp=[text, label],\n",
    "                                         Tout=(tf.int64, tf.int64))\n",
    "\n",
    "    # `tf.data.Datasets` work best if all components have a shape set\n",
    "    #  so set the shapes manually:\n",
    "    encoded_text.set_shape([None])\n",
    "    label.set_shape([])\n",
    "\n",
    "    return encoded_text, label\n",
    "\n",
    "\n",
    "all_encoded_data = all_labeled_data.map(encode_map_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_YZToSXSm0qr"
   },
   "source": [
    "## Split the dataset into test and train batches\n",
    "\n",
    "We can use `tf.data.Dataset.take` and `tf.data.Dataset.skip` to create a small test dataset and a larger training set.\n",
    "\n",
    "Before being passed into the model, the datasets need to be batched. Typically, the examples inside of a batch need to be the same size and shape. But, the examples in these datasets are not all the same size — each line of text had a different number of words. So use `tf.data.Dataset.padded_batch` (instead of `batch`) to pad the examples to the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-rmbijQh6bf"
   },
   "outputs": [],
   "source": [
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
    "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE)\n",
    "test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([None],[]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xdz7SVwmqi1l"
   },
   "source": [
    "Now, `test_data` and `train_data` are not collections of (`example, label`) pairs, but collections of batches. Each batch is a pair of (*many examples*, *many labels*) represented as arrays.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMslWfuwoqpB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(14,), dtype=int64, numpy=\n",
       " array([ 5057, 10197,  6221,   164,  2273,   841,  9758,  4349, 15865,\n",
       "        11355, 10608,     0,     0,     0])>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "sample_text[0], sample_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UI4I6_Sa0vWu"
   },
   "source": [
    "Since we have introduced a new token encoding (the zero used for padding), the vocabulary size has increased by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlD1Lli91vuc"
   },
   "outputs": [],
   "source": [
    "vocab_size += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8SUhGFNsmRi"
   },
   "source": [
    "## Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJgI1pow2YR9"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi0iiKLTKdoF"
   },
   "source": [
    "The first layer converts integer representations to dense vector embeddings. Here we will use the embedding layer, with randomly initilized values which we will train during our training. (Note in practice we would often use a pre-trained word embedding as described above, or use a pre-trained word embedding model and fine tune it during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DR6-ctbY638P"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Embedding(vocab_size, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8OJOPohKh1q"
   },
   "source": [
    "The next layer is a Long Short-Term Memory layer, which lets the model understand words in their context with other words. A bidirectional wrapper on the LSTM helps it to learn about the datapoints in relationship to the datapoints that came before it and after it. (We will go into more detail in an upcoming video on the LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6rnq6DN_WUs"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdffbMr5LF1g"
   },
   "source": [
    "Finally we'll have a series of one or more densely connected layers, with the last one being the output layer. The output layer produces a probability for all the labels. The one with the highest probability is the models prediction of an example's label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTEaNSnLCsv5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          1099456   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,178,115\n",
      "Trainable params: 1,178,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# One or more dense layers.\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer. The first argument is the number of labels.\n",
    "model.add(tf.keras.layers.Dense(3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zLHPU8q5DLi_"
   },
   "source": [
    "Finally, compile the model. For a softmax categorization model, use `sparse_categorical_crossentropy` as the loss function. We will use our popular `Adam` optomizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pkTBUVO4h6Y5"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DM-HLo5NDhql"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "This model running on this data produces decent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLtO33tNh6V8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "697/697 [==============================] - 25s 36ms/step - loss: 0.5301 - accuracy: 0.7349 - val_loss: 0.4365 - val_accuracy: 0.7906\n",
      "Epoch 2/3\n",
      "697/697 [==============================] - 22s 32ms/step - loss: 0.3018 - accuracy: 0.8685 - val_loss: 0.3680 - val_accuracy: 0.8348\n",
      "Epoch 3/3\n",
      "697/697 [==============================] - 23s 33ms/step - loss: 0.2223 - accuracy: 0.9036 - val_loss: 0.3997 - val_accuracy: 0.8344\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, epochs=3, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTPCYf_Jh6TH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     79/Unknown - 2s 27ms/step - loss: 0.3997 - accuracy: 0.8344\n",
      "Evaluation loss: 0.400, \n",
      "Evaluation accuracy: 0.834\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_acc = model.evaluate(test_data)\n",
    "\n",
    "print('\\nEvaluation loss: {:.3f}, \\nEvaluation accuracy: {:.3f}'.format(eval_loss, eval_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
