{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /Users/samholt/anaconda3/lib/python3.7/site-packages (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /Users/samholt/anaconda3/lib/python3.7/site-packages (from pydot) (2.4.5)\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy = True\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "sn.set(rc={'figure.figsize':(9,9)})\n",
    "sn.set(font_scale=1.4)\n",
    "\n",
    "# make results reproducible\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "!pip install pydot\n",
    "!rm -rf ./logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimators\n",
    "\n",
    "TensorFlow [Estimators](https://www.tensorflow.org/tutorials/estimator/premade) are a high-level representation of a complete model, which has been designed for easy scaling and asynchronous training. However it is recommended to use the Keras API instead, which has greater flexibility in creating a model, however we include estimators for completeness.\n",
    "\n",
    "An estimator encapsulates training, evaluation, prediction and export for serving. They have the benefit that the can easily be run on a local host machine or on a distributed multi-server environment, with CPU's, GPU's or TPU's without changing the model. They provide a safe distributed training loop that controls loading data, handling exceptions, creating model checkpoints to recover from failures and saving summaries for TensorBoard. When we work with estimators, we must separate the data input pipeline from the model.\n",
    "\n",
    "## Pre-made Estimators\n",
    "\n",
    "TensorFlow broadly supports the following estimators (from `tf.estimator`'s) for Classification\n",
    "* `BoostedTreesClassifier` : Boosted Trees model\n",
    "* `LinearClassifier` : Linear model\n",
    "* `DNNClassifier` : Deep neural network model\n",
    "* `DNNLinearCombinedClassifier` : Deep neural network and linear joined model \n",
    "\n",
    "And similarly for regression\n",
    "* `BoostedTreesRegressor` : Boosted Trees model\n",
    "* `LinearRegressor` : Linear model\n",
    "* `DNNRegressor` : Deep neural network model\n",
    "* `DNNLinearCombinedRegressor` : Deep neural network and linear joined model \n",
    "\n",
    "## Automatic data pre-processing\n",
    "\n",
    "One of the benefits that Estimators have is that we specify what data type our features are and it will correctly pre-process the features for that model, i.e `numeric`, `categorical` or `string` based data. As with Keras we would have to pre-process our features before passing them to the model. We often want to turn `categorical` features into an encoding as *one-hot* representation and use an embedding to map `strings` either categorical representation (*one-hot* encodings) or some other representation such as numeric vector.\n",
    "\n",
    "[Some](https://www.tensorflow.org/api_docs/python/tf/feature_column) of the datatypes, `tf.feature_column` that we can specify are:\n",
    "* `numeric_column` : numeric features\n",
    "* `categorical_column_with_vocabulary_list` : `CategoricalColumn` with a vocabulary list\n",
    "* `embedding_column` : converts sparse input to categorical input\n",
    "* `bucketized_column` : discretized dense input bucketed by `boundaries`\n",
    "\n",
    "## Setup\n",
    "\n",
    "To use estimators we have to pass a dataset loading function during training, evaluation and predicting. This function must return a tuple of two objects:\n",
    "* Dictionary, of keys of *feature_name*'s, with corresponding values of *feature_tensor*'s\n",
    "* Targets, tensor\n",
    "\n",
    "For example on our digits dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading our digits dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "(X, y) = datasets.load_digits(return_X_y=True)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pre process the data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create the datset loading functions\n",
    "def input_train_fn():\n",
    "    return {'X': X_train}, y_train\n",
    "\n",
    "def input_test_fn():\n",
    "    return {'X': X_test}, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to specify to the estimator what type of data is each feature is using the `tf.feature_column` types. In this example we have numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column('X', shape=[64])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instantiate the estimator, here we will use the `tf.estimator.DNNClassifier` for our classification problem.\n",
    "\n",
    "\n",
    "For the `tf.estimator.DNNClassifier` [some](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier) of the keyword arguments it takes are:\n",
    "* `hidden_units` : List of number of hidden units for each fully connected layer\n",
    "* `feature_columns` : List of all of the `tf.feature_column` types for the input features\n",
    "* `n_classes` : Number of classes\n",
    "* `optimizer` : Instance of `tf.keras.optimizer.*` used to train the model, can also be a built-in string\n",
    "* `activation_function` : Activation function to use for each layer, defaults to `tf.nn.relu`\n",
    "* `dropuout` : Probability to drop a given neuron during training, defaults to None\n",
    "* `batch_norm` : Whether to use batch normalization after each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/mg/rx49509d49q70jr6f72nnrbc0000gn/T/tmpvyvd2us3\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/mg/rx49509d49q70jr6f72nnrbc0000gn/T/tmpvyvd2us3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Specify how many layers and what hidden units they have are\n",
    "hidden_units = [64, 64, 64, 10]\n",
    "\n",
    "# Number of classes\n",
    "n_classes = 10\n",
    "\n",
    "# Create the estimator\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=hidden_units,\n",
    "    n_classes=n_classes,\n",
    "    optimizer='Adam',\n",
    "    dropout=0.1,\n",
    "    batch_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train our model passing the function to load the training dataset in the expected format. Note here we don't have epochs instead we have steps, which is how many steps to train the model for. Here we can approximate epochs with the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/samholt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/samholt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/mg/rx49509d49q70jr6f72nnrbc0000gn/T/tmpvyvd2us3/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.7013602, step = 0\n",
      "INFO:tensorflow:global_step/sec: 173.027\n",
      "INFO:tensorflow:loss = 0.7558022, step = 100 (0.579 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.967\n",
      "INFO:tensorflow:loss = 0.35988194, step = 200 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.007\n",
      "INFO:tensorflow:loss = 0.20036392, step = 300 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.053\n",
      "INFO:tensorflow:loss = 0.13819052, step = 400 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.731\n",
      "INFO:tensorflow:loss = 0.10059221, step = 500 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.317\n",
      "INFO:tensorflow:loss = 0.07419499, step = 600 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.49\n",
      "INFO:tensorflow:loss = 0.0631024, step = 700 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.806\n",
      "INFO:tensorflow:loss = 0.052611392, step = 800 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.873\n",
      "INFO:tensorflow:loss = 0.051402986, step = 900 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.438\n",
      "INFO:tensorflow:loss = 0.03914232, step = 1000 (0.421 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.276\n",
      "INFO:tensorflow:loss = 0.032135293, step = 1100 (0.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.19\n",
      "INFO:tensorflow:loss = 0.030701509, step = 1200 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.714\n",
      "INFO:tensorflow:loss = 0.034469694, step = 1300 (0.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.492\n",
      "INFO:tensorflow:loss = 0.036709297, step = 1400 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.709\n",
      "INFO:tensorflow:loss = 0.02255518, step = 1500 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.584\n",
      "INFO:tensorflow:loss = 0.021740498, step = 1600 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.032\n",
      "INFO:tensorflow:loss = 0.030482067, step = 1700 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.369\n",
      "INFO:tensorflow:loss = 0.02871251, step = 1800 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.747\n",
      "INFO:tensorflow:loss = 0.023958316, step = 1900 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.237\n",
      "INFO:tensorflow:loss = 0.029063197, step = 2000 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.192\n",
      "INFO:tensorflow:loss = 0.017076174, step = 2100 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.918\n",
      "INFO:tensorflow:loss = 0.015600182, step = 2200 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.601\n",
      "INFO:tensorflow:loss = 0.02186555, step = 2300 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.725\n",
      "INFO:tensorflow:loss = 0.017393207, step = 2400 (0.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.532\n",
      "INFO:tensorflow:loss = 0.014643699, step = 2500 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.35\n",
      "INFO:tensorflow:loss = 0.015106648, step = 2600 (0.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.379\n",
      "INFO:tensorflow:loss = 0.019190995, step = 2700 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.244\n",
      "INFO:tensorflow:loss = 0.027401961, step = 2800 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.937\n",
      "INFO:tensorflow:loss = 0.014118784, step = 2900 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.5\n",
      "INFO:tensorflow:loss = 0.019777354, step = 3000 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.848\n",
      "INFO:tensorflow:loss = 0.011480769, step = 3100 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.702\n",
      "INFO:tensorflow:loss = 0.016126454, step = 3200 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.038\n",
      "INFO:tensorflow:loss = 0.017983967, step = 3300 (0.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.616\n",
      "INFO:tensorflow:loss = 0.016339695, step = 3400 (0.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.211\n",
      "INFO:tensorflow:loss = 0.010426676, step = 3500 (0.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.495\n",
      "INFO:tensorflow:loss = 0.015285561, step = 3600 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 238.623\n",
      "INFO:tensorflow:loss = 0.015163859, step = 3700 (0.419 sec)\n",
      "INFO:tensorflow:global_step/sec: 240\n",
      "INFO:tensorflow:loss = 0.01232542, step = 3800 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.004\n",
      "INFO:tensorflow:loss = 0.0077429456, step = 3900 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.301\n",
      "INFO:tensorflow:loss = 0.012205245, step = 4000 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.31\n",
      "INFO:tensorflow:loss = 0.008906062, step = 4100 (0.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.398\n",
      "INFO:tensorflow:loss = 0.009302081, step = 4200 (0.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.976\n",
      "INFO:tensorflow:loss = 0.021661755, step = 4300 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 225.138\n",
      "INFO:tensorflow:loss = 0.013765384, step = 4400 (0.444 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.792\n",
      "INFO:tensorflow:loss = 0.005711852, step = 4500 (0.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.572\n",
      "INFO:tensorflow:loss = 0.014516703, step = 4600 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.822\n",
      "INFO:tensorflow:loss = 0.010034909, step = 4700 (0.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.417\n",
      "INFO:tensorflow:loss = 0.011012944, step = 4800 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.271\n",
      "INFO:tensorflow:loss = 0.0058419057, step = 4900 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.935\n",
      "INFO:tensorflow:loss = 0.006046965, step = 5000 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 240.421\n",
      "INFO:tensorflow:loss = 0.012888775, step = 5100 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.585\n",
      "INFO:tensorflow:loss = 0.009775381, step = 5200 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.055\n",
      "INFO:tensorflow:loss = 0.011859468, step = 5300 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.12\n",
      "INFO:tensorflow:loss = 0.020752141, step = 5400 (0.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.885\n",
      "INFO:tensorflow:loss = 0.007274754, step = 5500 (0.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.407\n",
      "INFO:tensorflow:loss = 0.0027633354, step = 5600 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.869\n",
      "INFO:tensorflow:loss = 0.00677403, step = 5700 (0.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.921\n",
      "INFO:tensorflow:loss = 0.010498753, step = 5800 (0.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.985\n",
      "INFO:tensorflow:loss = 0.010352173, step = 5900 (0.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.383\n",
      "INFO:tensorflow:loss = 0.0059300438, step = 6000 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.02\n",
      "INFO:tensorflow:loss = 0.011223089, step = 6100 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.74\n",
      "INFO:tensorflow:loss = 0.015161237, step = 6200 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.092\n",
      "INFO:tensorflow:loss = 0.014186974, step = 6300 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.287\n",
      "INFO:tensorflow:loss = 0.016640313, step = 6400 (0.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.26\n",
      "INFO:tensorflow:loss = 0.007006618, step = 6500 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.745\n",
      "INFO:tensorflow:loss = 0.008672068, step = 6600 (0.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 239.141\n",
      "INFO:tensorflow:loss = 0.0063893977, step = 6700 (0.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.393\n",
      "INFO:tensorflow:loss = 0.006433106, step = 6800 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.454\n",
      "INFO:tensorflow:loss = 0.012579614, step = 6900 (0.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.347\n",
      "INFO:tensorflow:loss = 0.004706038, step = 7000 (0.406 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 242.767\n",
      "INFO:tensorflow:loss = 0.0025985076, step = 7100 (0.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.783\n",
      "INFO:tensorflow:loss = 0.010720789, step = 7200 (0.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.634\n",
      "INFO:tensorflow:loss = 0.005264971, step = 7300 (0.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.609\n",
      "INFO:tensorflow:loss = 0.0034968024, step = 7400 (0.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.002\n",
      "INFO:tensorflow:loss = 0.011892931, step = 7500 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.644\n",
      "INFO:tensorflow:loss = 0.005103878, step = 7600 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.595\n",
      "INFO:tensorflow:loss = 0.010251191, step = 7700 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.659\n",
      "INFO:tensorflow:loss = 0.0074738627, step = 7800 (0.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.337\n",
      "INFO:tensorflow:loss = 0.009060824, step = 7900 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.402\n",
      "INFO:tensorflow:loss = 0.009328728, step = 8000 (0.411 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.468\n",
      "INFO:tensorflow:loss = 0.015122291, step = 8100 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.603\n",
      "INFO:tensorflow:loss = 0.0050218636, step = 8200 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.338\n",
      "INFO:tensorflow:loss = 0.008824354, step = 8300 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.981\n",
      "INFO:tensorflow:loss = 0.010489188, step = 8400 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.112\n",
      "INFO:tensorflow:loss = 0.014784629, step = 8500 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.193\n",
      "INFO:tensorflow:loss = 0.010925248, step = 8600 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.468\n",
      "INFO:tensorflow:loss = 0.0031065552, step = 8700 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.754\n",
      "INFO:tensorflow:loss = 0.008864734, step = 8800 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.488\n",
      "INFO:tensorflow:loss = 0.0072128763, step = 8900 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.147\n",
      "INFO:tensorflow:loss = 0.005267534, step = 9000 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.135\n",
      "INFO:tensorflow:loss = 0.006435829, step = 9100 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.222\n",
      "INFO:tensorflow:loss = 0.006738968, step = 9200 (0.390 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.612\n",
      "INFO:tensorflow:loss = 0.0043883403, step = 9300 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.653\n",
      "INFO:tensorflow:loss = 0.008894386, step = 9400 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.428\n",
      "INFO:tensorflow:loss = 0.0033551017, step = 9500 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.8\n",
      "INFO:tensorflow:loss = 0.0087200515, step = 9600 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.67\n",
      "INFO:tensorflow:loss = 0.0125978375, step = 9700 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 244.702\n",
      "INFO:tensorflow:loss = 0.014642806, step = 9800 (0.409 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.514\n",
      "INFO:tensorflow:loss = 0.007913845, step = 9900 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.019\n",
      "INFO:tensorflow:loss = 0.003096123, step = 10000 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.783\n",
      "INFO:tensorflow:loss = 0.003794738, step = 10100 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.997\n",
      "INFO:tensorflow:loss = 0.006209057, step = 10200 (0.405 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.197\n",
      "INFO:tensorflow:loss = 0.0042184913, step = 10300 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.325\n",
      "INFO:tensorflow:loss = 0.0031703839, step = 10400 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.872\n",
      "INFO:tensorflow:loss = 0.009344737, step = 10500 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.366\n",
      "INFO:tensorflow:loss = 0.0059425817, step = 10600 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.85\n",
      "INFO:tensorflow:loss = 0.004152871, step = 10700 (0.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 243.749\n",
      "INFO:tensorflow:loss = 0.0042037745, step = 10800 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 252.696\n",
      "INFO:tensorflow:loss = 0.0081831375, step = 10900 (0.396 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.696\n",
      "INFO:tensorflow:loss = 0.010674429, step = 11000 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.606\n",
      "INFO:tensorflow:loss = 0.0023589006, step = 11100 (0.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.463\n",
      "INFO:tensorflow:loss = 0.0021804548, step = 11200 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.508\n",
      "INFO:tensorflow:loss = 0.0036653809, step = 11300 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.847\n",
      "INFO:tensorflow:loss = 0.0056327474, step = 11400 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.068\n",
      "INFO:tensorflow:loss = 0.009327609, step = 11500 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.738\n",
      "INFO:tensorflow:loss = 0.011722249, step = 11600 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.31\n",
      "INFO:tensorflow:loss = 0.0026572666, step = 11700 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 245.466\n",
      "INFO:tensorflow:loss = 0.007874557, step = 11800 (0.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 256.049\n",
      "INFO:tensorflow:loss = 0.004602914, step = 11900 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.096\n",
      "INFO:tensorflow:loss = 0.0043476275, step = 12000 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 248.362\n",
      "INFO:tensorflow:loss = 0.00416465, step = 12100 (0.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.49\n",
      "INFO:tensorflow:loss = 0.002904619, step = 12200 (0.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 237.49\n",
      "INFO:tensorflow:loss = 0.0059567792, step = 12300 (0.421 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.237\n",
      "INFO:tensorflow:loss = 0.0070662177, step = 12400 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 253.467\n",
      "INFO:tensorflow:loss = 0.0018059599, step = 12500 (0.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.375\n",
      "INFO:tensorflow:loss = 0.0032198841, step = 12600 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.631\n",
      "INFO:tensorflow:loss = 0.0019612594, step = 12700 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.442\n",
      "INFO:tensorflow:loss = 0.0027196084, step = 12800 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.967\n",
      "INFO:tensorflow:loss = 0.0047452464, step = 12900 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.472\n",
      "INFO:tensorflow:loss = 0.0023392567, step = 13000 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.774\n",
      "INFO:tensorflow:loss = 0.0018819162, step = 13100 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.697\n",
      "INFO:tensorflow:loss = 0.0033633697, step = 13200 (0.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.802\n",
      "INFO:tensorflow:loss = 0.0035861237, step = 13300 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.509\n",
      "INFO:tensorflow:loss = 0.002696058, step = 13400 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 250.116\n",
      "INFO:tensorflow:loss = 0.006056179, step = 13500 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.482\n",
      "INFO:tensorflow:loss = 0.0014526875, step = 13600 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.864\n",
      "INFO:tensorflow:loss = 0.0044265194, step = 13700 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.148\n",
      "INFO:tensorflow:loss = 0.0023938096, step = 13800 (0.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.192\n",
      "INFO:tensorflow:loss = 0.0063305977, step = 13900 (0.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.9\n",
      "INFO:tensorflow:loss = 0.0025866916, step = 14000 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.917\n",
      "INFO:tensorflow:loss = 0.010140967, step = 14100 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 264.786\n",
      "INFO:tensorflow:loss = 0.012447982, step = 14200 (0.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 260.316\n",
      "INFO:tensorflow:loss = 0.002828758, step = 14300 (0.384 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14370 into /var/folders/mg/rx49509d49q70jr6f72nnrbc0000gn/T/tmpvyvd2us3/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0027002955.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x7ff6d8c6f550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "steps = epochs * X_train.shape[0]\n",
    "\n",
    "classifier.train(input_fn=input_train_fn,\n",
    "                steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can then pass a function to load our test dataset, and evaluate our model, averaging the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-04-27T02:39:31Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/mg/rx49509d49q70jr6f72nnrbc0000gn/T/tmpvyvd2us3/model.ckpt-14370\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Inference Time : 0.37566s\n",
      "INFO:tensorflow:Finished evaluation at 2020-04-27-02:39:31\n",
      "INFO:tensorflow:Saving dict for global step 14370: accuracy = 0.98333335, average_loss = 0.31491718, global_step = 14370, loss = 0.31491706\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 14370: /var/folders/mg/rx49509d49q70jr6f72nnrbc0000gn/T/tmpvyvd2us3/model.ckpt-14370\n",
      "{'accuracy': 0.98333335, 'average_loss': 0.31491718, 'loss': 0.31491706, 'global_step': 14370}\n",
      "\n",
      "Test accuracy: 98.333%\n"
     ]
    }
   ],
   "source": [
    "eval_result = classifier.evaluate(input_fn=input_test_fn, steps=100)\n",
    "print(eval_result)\n",
    "print('\\nTest accuracy: {:.3%}'.format(eval_result['accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have somewhat easily trained a simple classification model with a sufficient test accuracy.\n",
    "\n",
    "Can easily make predictions using our trained model as well using the `tf.estimator.DNNClassifier.predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard\n",
    "\n",
    "Is TensorFlow's visualization [toolkit](https://www.tensorflow.org/tensorboard/get_started), that automatically installs when we installed TensorFlow. It is commonly used to visualize:\n",
    "* metrics such as loss and accuracy during training\n",
    "* the model graph (operations and layers)\n",
    "* histograms of weights, biases and other tensors that change over time\n",
    "* projecting embeddings to a lower dimensional space to see the structure\n",
    "* profiling tensorflow models and programs\n",
    "* view the best hyperparameters over a search in a parallel co-ordinate view\n",
    "* and more features continually added (What-If-Tool, Fairness indicators etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use an example dataset, here the MNIST dataset to create a simple model in Keras which we can visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save TensorBoard viewable logs, we need to pass the callback `tf.keras.callbacks.TensorBoard` to our `Model.fit()` in Keras. We can also enable histogram saving every epoch with `histogram_freq=1`.\n",
    "\n",
    "It is common practice to put the logs in a timestamped subdirectory to allow easy selection of different training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.2218 - accuracy: 0.9343 - val_loss: 0.1072 - val_accuracy: 0.9666\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.0976 - accuracy: 0.9705 - val_loss: 0.0827 - val_accuracy: 0.9728\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.0698 - accuracy: 0.9775 - val_loss: 0.0837 - val_accuracy: 0.9755\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.0531 - accuracy: 0.9826 - val_loss: 0.0709 - val_accuracy: 0.9795\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0429 - accuracy: 0.9857 - val_loss: 0.0604 - val_accuracy: 0.9827\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=5, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start TensorBoard inside the notebook or from the command line, therefore we run for:\n",
    "* Notebook: `%tensorboard --logdir logs/fit`\n",
    "* Command Line: `tensorboard --logdir logs/fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2db54b456d64eaa3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2db54b456d64eaa3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the dashboards above (in the above tabs):\n",
    "* The **Scalars** tab plots the metrics we defined when we compiled our model, alongside any validation values if we have a validation dataset we are training with. This can also track any other scalar, such as learning rates etc.\n",
    "* The **Graphs** tab visualizes our model. Which helps to check that we constructed it correctly.\n",
    "* The **Distributions** and **Histograms** tabs show the distribution for a tensor for every epoch. This can be useful to verify to see that the weights and biases are changing the expected way over each epoch.\n",
    "\n",
    "Also extra TensorBoard plugins are automatically enabled when we log other types of data as well. For example, the [Keras TensorBoard callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) lets us log images and embeddings as well. We can see what other plugins that TensorBoard supports that are currently inactive by the tab dropdown in the top right."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
