{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy = True\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "# !sudo apt-get install -y xvfb ffmpeg\n",
    "# !pip install 'xvfbwrapper==0.2.9'\n",
    "# !pip install 'gym==0.10.11'\n",
    "# !pip install 'imageio==2.4.0'\n",
    "# !pip install PILLOW\n",
    "# !pip install 'pyglet==1.3.2'\n",
    "# !pip install pyvirtualdisplay\n",
    "# !pip install tf-agents\n",
    "# !pip install gast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "\n",
    "import abc\n",
    "import base64\n",
    "import imageio\n",
    "import io\n",
    "import IPython\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import shutil\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "sn.set(rc={'figure.figsize':(9,9)})\n",
    "sn.set(font_scale=1.4)\n",
    "\n",
    "# make results reproducible\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value and Policy based methods\n",
    "\n",
    "So far we have looked at value based methods, or approaches to reinforcement learning, such as DQN methods. Let us now look at policy based methods, such as PPO.\n",
    "\n",
    "\n",
    "# Value Based Methods\n",
    "\n",
    "Learn a harder problem, epilson greedy exploration\n",
    "\n",
    "* Vanilla DQN `DqnAgent`\n",
    "* PDD DQN `DdpgAgent`\n",
    "* Double Q-Learning ?\n",
    "* SARSA\n",
    "* Value-iteration ?\n",
    "\n",
    "# Policy Based Methods \n",
    "\n",
    "Learn an easier problem\n",
    "\n",
    "* Reinforce here ! \n",
    "* A2C ``\n",
    "* PPO `PPOAgent`\n",
    "\n",
    "# Off-policy Policy Based Methods\n",
    "\n",
    "* SAC `SacAgent`\n",
    "* SIL (not with A2C, PPO but SAC)\n",
    "\n",
    "# Exploration Techniques \n",
    "\n",
    "* Thompson sampling with MCDO\n",
    "* RND\n",
    "\n",
    "#  Uncertainty in RL\n",
    "\n",
    "* Categorical DQN(C51) `CategoricalDqnAgent`\n",
    "* QR-DQN\n",
    "* Implicit Quantile Networks\n",
    "\n",
    "#  Imitation Learning \n",
    "* GAIL\n",
    "\n",
    "# Multi-Agent RL\n",
    "* Upper Confidence Bounds for Tree(UCT)\n",
    "* Counterfactual Hedge\n",
    "\n",
    "\n",
    "**Main Networks**\n",
    "\n",
    "* **QNetwork**: Used in Qlearning for environments with discrete actions, this network maps an observation to value estimates for each possible action.\n",
    "* **CriticNetworks**: Also referred to as `ValueNetworks` in literature, learns to estimate some version of a Value function mapping some state into an estimate for the expected return of a policy. These networks estimate how good the state the agent is currently in is.\n",
    "* **ActorNetworks**: Learn a mapping from observations to actions. These networks are usually used by our policies to generate actions.\n",
    "* **ActorDistributionNetworks**: Similar to `ActorNetworks` but these generate a distribution which a policy can then sample to generate actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.behavioral_cloning.behavioral_cloning_agent import BehavioralCloningAgent\n",
    "from tf_agents.agents.categorical_dqn.categorical_dqn_agent import CategoricalDqnAgent\n",
    "from tf_agents.agents.ddpg.ddpg_agent import DdpgAgent\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.agents.ppo.ppo_agent import PPOAgent\n",
    "from tf_agents.agents.reinforce.reinforce_agent import ReinforceAgent\n",
    "from tf_agents.agents.sac.sac_agent import SacAgent\n",
    "from tf_agents.agents.td3.td3_agent import Td3Agent\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
