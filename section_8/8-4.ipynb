{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
      "xvfb is already the newest version (2:1.19.6-1ubuntu4.4).\n",
      "The following packages were automatically installed and are no longer required:\n",
      "  libaio1 librados2 librbd1\n",
      "Use 'sudo apt autoremove' to remove them.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.\n",
      "Requirement already satisfied: xvfbwrapper==0.2.9 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (0.2.9)\n",
      "Requirement already satisfied: gym==0.10.11 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (0.10.11)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym==0.10.11) (1.4.1)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym==0.10.11) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym==0.10.11) (1.18.1)\n",
      "Requirement already satisfied: requests>=2.0 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym==0.10.11) (2.23.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from gym==0.10.11) (1.3.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.0->gym==0.10.11) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.0->gym==0.10.11) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.0->gym==0.10.11) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.0->gym==0.10.11) (2020.4.5.1)\n",
      "Requirement already satisfied: future in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from pyglet>=1.2.0->gym==0.10.11) (0.18.2)\n",
      "Requirement already satisfied: imageio==2.4.0 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (2.4.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from imageio==2.4.0) (1.18.1)\n",
      "Requirement already satisfied: pillow in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from imageio==2.4.0) (7.0.0)\n",
      "Requirement already satisfied: PILLOW in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (7.0.0)\n",
      "Requirement already satisfied: pyglet==1.3.2 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (1.3.2)\n",
      "Requirement already satisfied: future in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from pyglet==1.3.2) (0.18.2)\n",
      "Requirement already satisfied: pyvirtualdisplay in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (0.2.5)\n",
      "Requirement already satisfied: EasyProcess in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from pyvirtualdisplay) (0.3)\n",
      "Requirement already satisfied: tf-agents in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (0.4.0)\n",
      "Requirement already satisfied: absl-py>=0.6.1 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tf-agents) (0.9.0)\n",
      "Requirement already satisfied: gin-config==0.1.3 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tf-agents) (0.1.3)\n",
      "Requirement already satisfied: tensorflow-probability>=0.8.0 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tf-agents) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tf-agents) (1.18.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tf-agents) (1.14.0)\n",
      "Requirement already satisfied: gast>=0.3.2 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-probability>=0.8.0->tf-agents) (0.3.3)\n",
      "Requirement already satisfied: decorator in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-probability>=0.8.0->tf-agents) (4.4.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.2 in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-probability>=0.8.0->tf-agents) (1.3.0)\n",
      "Requirement already satisfied: gast in /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (0.3.3)\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy = True\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "!sudo apt-get install -y xvfb ffmpeg\n",
    "!pip install 'xvfbwrapper==0.2.9'\n",
    "!pip install 'gym==0.10.11'\n",
    "!pip install 'imageio==2.4.0'\n",
    "!pip install PILLOW\n",
    "!pip install 'pyglet==1.3.2'\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install tf-agents\n",
    "!pip install gast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "\n",
    "import abc\n",
    "import base64\n",
    "import imageio\n",
    "import io\n",
    "import IPython\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import shutil\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "sn.set(rc={'figure.figsize':(9,9)})\n",
    "sn.set(font_scale=1.4)\n",
    "\n",
    "# make results reproducible\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(13)\n",
    "\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "attachments": {
    "rl_algorithms_9_15.svg": {
     "image/svg+xml": [
      "<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="987px" height="511px" version="1.1" content="&lt;mxfile userAgent=&quot;Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:62.0) Gecko/20100101 Firefox/62.0&quot; version=&quot;9.1.3&quot; editor=&quot;www.draw.io&quot; type=&quot;device&quot;&gt;&lt;diagram name=&quot;Page-1&quot; id=&quot;8ce9d11a-91a2-4d17-14d8-a56ed91bf033&quot;&gt;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&lt;/diagram&gt;&lt;/mxfile&gt;"><defs/><g transform="translate(0.5,0.5)"><path d="M 493 70 L 493 84.38" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 493 88.88 L 491 82.88 L 493 84.38 L 495 82.88 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 492.5 90 L 591 90 Q 601 90 601 97.19 L 601 104.38" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 601 108.88 L 599 102.88 L 601 104.38 L 603 102.88 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 492.5 90 L 398 90 Q 388 90 388 97.19 L 388 104.38" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 388 108.88 L 386 102.88 L 388 104.38 L 390 102.88 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><rect x="312.5" y="110" width="150" height="60" rx="9" ry="9" fill="#ffe6cc" stroke="#d79b00" pointer-events="none"/><g transform="translate(329.5,131.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="116" height="17" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 117px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 16px">Model-Free RL</font></div></div></foreignObject><text x="58" y="15" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="417.5" y="10" width="150" height="60" rx="9" ry="9" fill="#ffe6cc" stroke="#d79b00" pointer-events="none"/><g transform="translate(440.5,31.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="104" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 105px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">RL Algorithms</font></div></div></foreignObject><text x="52" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="525.5" y="110" width="150" height="60" rx="9" ry="9" fill="#ffe6cc" stroke="#d79b00" pointer-events="none"/><g transform="translate(540.5,131.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="119" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 120px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">Model-Based RL</font></div></div></foreignObject><text x="60" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><path d="M 388 170 L 388 180 Q 388 190 378 190 L 293 190 Q 283 190 282.82 197.19 L 282.64 204.38" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 282.53 208.88 L 280.68 202.83 L 282.64 204.38 L 284.68 202.93 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 282.5 210 L 378 210 Q 388 210 388 217.19 L 388 224.38" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 388 228.88 L 386 222.88 L 388 224.38 L 390 222.88 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 282.5 210 L 185 210 Q 175 210 175 217.19 L 175 224.38" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 175 228.88 L 173 222.88 L 175 224.38 L 177 222.88 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 175 290 L 175 330 L 137.37 330" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 132.12 330 L 139.12 326.5 L 137.37 330 L 139.12 333.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 175 290 L 175 380 L 137.37 380" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 132.12 380 L 139.12 376.5 L 137.37 380 L 139.12 383.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 175 290 L 175 430 L 137.37 430" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 132.12 430 L 139.12 426.5 L 137.37 430 L 139.12 433.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 175 290 L 175 480 L 137.37 480" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 132.12 480 L 139.12 476.5 L 137.37 480 L 139.12 483.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 175 290 L 175 350 L 215.63 350" fill="none" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><path d="M 220.88 350 L 213.88 353.5 L 215.63 350 L 213.88 346.5 Z" fill="#454545" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><path d="M 175 290 L 175 400 L 215.63 400" fill="none" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><path d="M 220.88 400 L 213.88 403.5 L 215.63 400 L 213.88 396.5 Z" fill="#454545" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><path d="M 175 290 L 175 450 L 215.63 450" fill="none" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><path d="M 220.88 450 L 213.88 453.5 L 215.63 450 L 213.88 446.5 Z" fill="#454545" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><rect x="99.5" y="230" width="150" height="60" rx="9" ry="9" fill="#ffe6cc" stroke="#d79b00" pointer-events="none"/><g transform="translate(102.5,251.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="144" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 145px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">Policy Optimization</font></div></div></foreignObject><text x="72" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">&lt;font style="font-size: 15px"&gt;Policy Optimization&lt;/font&gt;</text></switch></g><path d="M 388 290 L 388 330 L 427.63 330" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 432.88 330 L 425.88 333.5 L 427.63 330 L 425.88 326.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 388 290 L 388 380 L 427.63 380" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 432.88 380 L 425.88 383.5 L 427.63 380 L 425.88 376.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 388 290 L 388 430 L 427.63 430" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 432.88 430 L 425.88 433.5 L 427.63 430 L 425.88 426.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 388 290 L 388 480 L 427.13 480" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 432.38 480 L 425.38 483.5 L 427.13 480 L 425.38 476.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 388 290 L 388 350 L 348.37 350" fill="none" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><path d="M 343.12 350 L 350.12 346.5 L 348.37 350 L 350.12 353.5 Z" fill="#454545" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><path d="M 388 290 L 388 400 L 348.37 400" fill="none" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><path d="M 343.12 400 L 350.12 396.5 L 348.37 400 L 350.12 403.5 Z" fill="#454545" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><path d="M 388 290 L 388 450 L 348.37 450" fill="none" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><path d="M 343.12 450 L 350.12 446.5 L 348.37 450 L 350.12 453.5 Z" fill="#454545" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><rect x="312.5" y="230" width="150" height="60" rx="9" ry="9" fill="#ffe6cc" stroke="#d79b00" pointer-events="none"/><g transform="translate(345.5,251.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="84" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 85px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">Q-Learning</font></div></div></foreignObject><text x="42" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="10.5" y="460" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(50.5,471.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="40" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 41px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">TRPO</font></div></div></foreignObject><text x="20" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><path d="M 601 170 L 601 180 Q 601 190 611 190 L 699 190 Q 709 190 708.82 197.19 L 708.64 204.38" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 708.53 208.88 L 706.68 202.83 L 708.64 204.38 L 710.68 202.93 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 709 210 L 804 210 Q 814 210 814 217.19 L 814 224.38" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 814 228.88 L 812 222.88 L 814 224.38 L 816 222.88 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 709 210 L 611 210 Q 601 210 601 217.19 L 601 224.38" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 601 228.88 L 599 222.88 L 601 224.38 L 603 222.88 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 601 290 L 601 330 L 638.63 330" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 643.88 330 L 636.88 333.5 L 638.63 330 L 636.88 326.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 601 290 L 601 380 L 638.63 380" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 643.88 380 L 636.88 383.5 L 638.63 380 L 636.88 376.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 601 290 L 601 430 L 638.63 430" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 643.88 430 L 636.88 433.5 L 638.63 430 L 636.88 426.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><rect x="526" y="230" width="150" height="60" rx="9" ry="9" fill="#ffe6cc" stroke="#d79b00" pointer-events="none"/><g transform="translate(540.5,251.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="121" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 122px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">Learn the Model</font></div></div></foreignObject><text x="61" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><path d="M 814 290 L 814 330 L 849.63 330" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 854.88 330 L 847.88 333.5 L 849.63 330 L 847.88 326.5 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><rect x="738.5" y="230" width="150" height="60" rx="9" ry="9" fill="#ffe6cc" stroke="#d79b00" pointer-events="none"/><g transform="translate(751.5,251.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="123" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 124px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">Given the Model</font></div></div></foreignObject><text x="62" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="645" y="360" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(691.5,371.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="26" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 27px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">I2A</font></div></div></foreignObject><text x="13" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="645" y="310" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(655.5,321.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="99" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 100px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">World Models</font></div></div></foreignObject><text x="50" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="856" y="310" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(877.5,321.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="76" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 77px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">AlphaZero</font></div></div></foreignObject><text x="38" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><path d="M 946 310 L 946 310" fill="none" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><path d="M 946 310 L 946 310 L 946 310 L 946 310 Z" fill="#000000" stroke="#000000" stroke-miterlimit="10" pointer-events="none"/><rect x="645" y="410" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(682.5,421.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="45" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 46px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">MBMF</font></div></div></foreignObject><text x="23" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="433.5" y="360" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(478.5,371.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 31px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">C51</font></div></div></foreignObject><text x="15" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="433.5" y="410" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(461.5,421.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="63" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 64px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">QR-DQN</font></div></div></foreignObject><text x="32" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="433.5" y="310" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(475.5,321.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="35" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 36px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">DQN</font></div></div></foreignObject><text x="18" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="433.5" y="460" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(478.5,471.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 31px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">HER</font></div></div></foreignObject><text x="15" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="10.5" y="410" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(55.5,421.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 31px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">PPO</font></div></div></foreignObject><text x="15" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="10.5" y="360" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(31.5,371.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="77" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 78px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">A2C / A3C</font></div></div></foreignObject><text x="39" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="10.5" y="310" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(13.5,321.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="113" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 114px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">Policy Gradient</font></div></div></foreignObject><text x="57" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="221.5" y="430" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(266.5,441.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="30" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 31px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">SAC</font></div></div></foreignObject><text x="15" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="221.5" y="380" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(265.5,391.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="31" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 32px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">TD3</font></div></div></foreignObject><text x="16" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="221.5" y="330" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(258.5,341.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="45" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 46px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">DDPG</font></div></div></foreignObject><text x="23" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><rect x="645" y="460" width="120" height="40" rx="6" ry="6" fill="#dae8fc" stroke="#6c8ebf" pointer-events="none"/><g transform="translate(683.5,471.5)"><switch><foreignObject style="overflow:visible;" pointer-events="all" width="42" height="16" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: inline-block; font-size: 12px; font-family: Verdana; color: rgb(0, 0, 0); line-height: 1.2; vertical-align: top; width: 43px; white-space: nowrap; overflow-wrap: normal; text-align: center;"><div xmlns="http://www.w3.org/1999/xhtml" style="display:inline-block;text-align:inherit;text-decoration:inherit;"><font style="font-size: 15px">MBVE</font></div></div></foreignObject><text x="21" y="14" fill="#000000" text-anchor="middle" font-size="12px" font-family="Verdana">[Not supported by viewer]</text></switch></g><path d="M 600.5 480 L 601 290" fill="none" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><path d="M 599.5 480 L 638.63 480" fill="none" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/><path d="M 643.88 480 L 636.88 483.5 L 638.63 480 L 636.88 476.5 Z" fill="#454545" stroke="#454545" stroke-miterlimit="10" pointer-events="none"/></g></svg>"
     ]
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value and Policy based methods\n",
    "\n",
    "Lets review the taxonomy of RL algorithms:\n",
    "\n",
    "![rl_algorithms_9_15.svg](attachment:rl_algorithms_9_15.svg)\n",
    "\n",
    "Our first categorisation for our choice of RL aglorithm, is if the agent will use a **model** (either given or learns one) of the world (enviornment), often a function that predicts state transitions and rewards. Models allow us to **plan**  by simulating ahead what a would happen given a range of inputs. Agents can then distill results from planning into a learned policy (E.g. AlphaZero). When this works, it can result in a substantial improvement in sample efficiency (compared to non-model aglorithms). However in practice a **model** is rarely known and instead has to learn a model only from experience, which leads to bias in the model can be exploited by the agent, resulting in an agent that performs well on the learned model, however poorly in the real enviorment. For this reason, training **model-based** learning is difficult.\n",
    "\n",
    "**Model-free** agents, don't use a model, however tend to be more popular and easier to implement, they come in two broad categorisations **Value** (Q-Learning) and **Policy** based methods.\n",
    "\n",
    "## Value based methods\n",
    "\n",
    "Often denoted as *Q-learning*, we learn an approximator $Q_\\theta(s,a)$ for the optimal action-value function, $Q^*(s,a)$. Often they use a objective function (Used as the cost to maximize to update the parameters on) based on the Bellman equation. This optomization is almost always performed **off-policy**, that allows each update to use data collected at any point during training, regardless of how the agent was choosing to explore the environment when the data was collected. The policy taken by the Q-learning agent is gien by:\n",
    "\n",
    "$$a(s) =  \\operatorname{arg max}_{a} Q(s, a)$$\n",
    "\n",
    "Q-Learning agents in TF-Agents are:\n",
    "* [DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) (Deep Q-Networks)\n",
    "    * `from tf_agents.agents.dqn.dqn_agent import DqnAgent`\n",
    "* [C51](https://arxiv.org/abs/1707.06887) (A DQN variation, that learns a distribution over return whose expectation is $Q^*$)\n",
    "    * `from tf_agents.agents.categorical_dqn.categorical_dqn_agent import CategoricalDqnAgent`\n",
    "\n",
    "## Policy based methods\n",
    "\n",
    "Also known as *Policy Optomization*, where we parameterise the policy $\\pi_\\theta(a|s)$ and we keep updating its paramters until we converge to a performance that is no longer increasing. These optomise the parameterise $\\theta$ either directly by gradient descent on the defined performance objective, often a **policy value** $\\rho(\\theta)$ (the expected reward we get when following $\\pi_\\theta$), or by maximising local approximations of $\\rho(\\theta)$. This often performed on **on-policy**, which restricts each update to only use data collected whil acting according to hte most recent version that policy $\\pi$. Policy optomization often involves learning a value function approximator $V_\\phi(s)$ for hte on-policy value function $V^{\\pi}(s)$, which gets used to update the policy.\n",
    "\n",
    "Policy based agents in TF-Agents are:\n",
    "* [REINFORCE](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) (Vanilla Policy Optomization)\n",
    "    * `from tf_agents.agents.reinforce.reinforce_agent import ReinforceAgent`\n",
    "* [A2C / A3C](https://arxiv.org/abs/1602.01783) (Synchronous Actor Critic / Asyncrhornous Actor Critic) Gradient ascent to directly maximize performance. Combines REINFORCE with a learned baseline (Critic) to improve stability of learning. Can be trained parallelized (asyncrhonous) or synchronous for both discrete and continous action spaces.\n",
    "    * `from tf_agents.agents.sac.sac_agent import SacAgent`, passing in a `critic_network_2` network keyword argument, as the second critic network to be used during optomization.\n",
    "* [PPO](https://arxiv.org/abs/1707.06347) (Proximal Policy Optimization Algorithms: Actor-Critic) Maximizes a surrogate objective function which gives a conservative esimate for how much $\\rho(\\theta)$ will change as a result of the update. This actor critic scheme which uses bounded updates to the policy in order to make the learning process very stable.\n",
    "    * `from tf_agents.agents.ppo.ppo_agent import PPOAgent`\n",
    "\n",
    "## Trade-offs between Policy and Value based methods\n",
    "\n",
    "The primary strength of policy optimization methods is that they are principled, in that we directly optimize for the thing we want (An optimal policy $\\pi$). This tends to make them stable and reliable. In contrast, value based methods (Q-learning methods) only indirectly optimize for agent performance, by training $Q_{\\theta}$ to satisfy a self-consistency equation (Bellman equation). There are many failure modes for this kind of learning, so it tends to be less stable. However Q-learning methods gain the advantage of being substantially more sample efficient when they do work, because they can reuse data more effectively than policy optimization techniques. Policy methods also perform best on continous action spaces.\n",
    "\n",
    "## Interpolating between Policy and Value based methods\n",
    "\n",
    "Policy optimization and Q-learning methods are not incompatible (and under some circumstances, it turns out, equivalent), and there exist a range of algorithms that live in between the two extremes. Algorithms that live on this spectrum are able to carefully trade-off between the strengths and weaknesses of either side.\n",
    "\n",
    "TF-Agents includes:\n",
    "* [DDPG](https://arxiv.org/abs/1509.02971) (Deep Deterministic Policy Gradient) An actor critic scheme for continuous action spaces which assumes that the policy is deterministic, and therefore it is able to use a replay buffer in order to improve sample efficiency.\n",
    "    * `from tf_agents.agents.ddpg.ddpg_agent import DdpgAgent`\n",
    "* [TD3](https://arxiv.org/pdf/1802.09477.pdf) Very similar to DDPG, i.e. an actor-critic for continuous action spaces, that uses a replay buffer in order to improve sample efficiency. TD3 uses two critic networks in order to mitigate the overestimation in the Q state-action value prediction, slows down the actor updates in order to increase stability and adds noise to actions while training the critic in order to smooth out the critic's predictions.\n",
    "    * `from tf_agents.agents.td3.td3_agent import Td3Agent`\n",
    "* [SAC](https://arxiv.org/abs/1801.01290) (Soft Actor-Critic) DDPG variant which uses stochastic policies, entropy regularization, and a few other tricks to stabilize learning and score higher than DDPG on standard benchmarks. Here optimizing a stochastic policy in an off-policy way. One of the key features of SAC is that it solves a maximum entropy reinforcement learning problem.\n",
    "    * `from tf_agents.agents.sac.sac_agent import SacAgent`\n",
    "\n",
    "# TF-Agents example\n",
    "\n",
    "Lets compare and train some of these agents on the [inverted pendulum swingup](https://gym.openai.com/envs/Pendulum-v0/) problem. In this version of the problem, the pendulum starts in a random position, and the goal is to swing it up so it stays upright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=500x500 at 0x7FA760B48E48>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.environments import suite_gym\n",
    "env_name = \"Pendulum-v0\"\n",
    "env = suite_gym.load(env_name)\n",
    "env.reset()\n",
    "# Lets see a possible starting state\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HC1kNrOsLSIZ"
   },
   "outputs": [],
   "source": [
    "from tf_agents.drivers import dynamic_step_driver\n",
    "\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "num_iterations = 250  # @param {type:\"integer\"}\n",
    "collect_episodes_per_iteration = 2  # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 2000  # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 25  # @param {type:\"integer\"}\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 50  # @param {type:\"integer\"}\n",
    "\n",
    "# Create the training and evaluation environments\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "# Metrics\n",
    "\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "# Data collection\n",
    "\n",
    "\n",
    "def collect_episode(environment, policy, num_episodes, replay_buffer):\n",
    "\n",
    "    episode_counter = 0\n",
    "    environment.reset()\n",
    "\n",
    "    while episode_counter < num_episodes:\n",
    "        time_step = environment.current_time_step()\n",
    "        action_step = policy.action(time_step)\n",
    "        next_time_step = environment.step(action_step.action)\n",
    "        traj = trajectory.from_transition(\n",
    "            time_step, action_step, next_time_step)\n",
    "\n",
    "        # Add trajectory to the replay buffer\n",
    "        replay_buffer.add_batch(traj)\n",
    "\n",
    "        if traj.is_boundary():\n",
    "            episode_counter += 1\n",
    "\n",
    "# Embed video\n",
    "\n",
    "\n",
    "def embed_mp4(filename):\n",
    "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "    video = open(filename, 'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "    return IPython.display.HTML(tag)\n",
    "\n",
    "\n",
    "def train_and_evaluate_agent(tf_agent, name='agent'):\n",
    "    tf_agent.initialize()\n",
    "\n",
    "    eval_policy = tf_agent.policy\n",
    "    collect_policy = tf_agent.collect_policy\n",
    "\n",
    "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=tf_agent.collect_data_spec,\n",
    "        batch_size=train_env.batch_size,\n",
    "        max_length=replay_buffer_capacity)\n",
    "\n",
    "    tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "    # Train\n",
    "    tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "    # Evaluate the agent's policy once before training.\n",
    "    avg_return = compute_avg_return(\n",
    "        eval_env, tf_agent.policy, num_eval_episodes)\n",
    "    returns = [(0, avg_return)]\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "        collect_episode(\n",
    "            train_env, tf_agent.collect_policy, collect_episodes_per_iteration, replay_buffer)\n",
    "\n",
    "        # Use data from the buffer and update the agent's network.\n",
    "        experience = replay_buffer.gather_all()\n",
    "        train_loss = tf_agent.train(experience)\n",
    "        replay_buffer.clear()\n",
    "\n",
    "        step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            print('[{0}] step = {1}: loss = {2}'.format(\n",
    "                iteration, step, train_loss.loss))\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            avg_return = compute_avg_return(\n",
    "                eval_env, tf_agent.policy, num_eval_episodes)\n",
    "            print('[{0}] step = {1}: Average Return = {2}'.format(\n",
    "                iteration, step, avg_return))\n",
    "            returns.append((step, avg_return))\n",
    "\n",
    "        if step >= num_iterations:\n",
    "            break\n",
    "\n",
    "    steps_list = [r[0] for r in returns]\n",
    "    rewards_list = [r[1] for r in returns]\n",
    "    plt.plot(steps_list, rewards_list)\n",
    "    plt.ylabel('Average Return')\n",
    "    plt.xlabel('Step')\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize policies for a few episodes\n",
    "    num_episodes = 3\n",
    "    video_filename = '{}.mp4'.format(name)\n",
    "    with imageio.get_writer(video_filename, fps=60) as video:\n",
    "        for _ in range(num_episodes):\n",
    "            time_step = eval_env.reset()\n",
    "            video.append_data(eval_py_env.render())\n",
    "            while not time_step.is_last():\n",
    "                action_step = tf_agent.policy.action(time_step)\n",
    "                time_step = eval_env.step(action_step.action)\n",
    "                video.append_data(eval_py_env.render())\n",
    "\n",
    "    return returns, video_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9lW_OZYFR8A"
   },
   "source": [
    "# Agents\n",
    "\n",
    "## REINFORCE\n",
    "\n",
    "Vanilla Policy Optomization, we expect other methods to outpeform this, this will need an `Actor Network` that can learn to predict the action given an observation from the environment.\n",
    "\n",
    "We can easily create an `Actor Network` using the specs of the observations and actions. We can specify the layers in the network which, in this example, is the `fc_layer_params` argument set to a tuple of `ints` representing the sizes of each hidden layer (see the Hyperparameters section above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TgkdEPg_muzV"
   },
   "outputs": [],
   "source": [
    "# from tf_agents.agents.reinforce import reinforce_agent\n",
    "\n",
    "# actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "#     train_env.observation_spec(),\n",
    "#     train_env.action_spec(),\n",
    "#     fc_layer_params=fc_layer_params)\n",
    "\n",
    "# optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "# tf_agent = reinforce_agent.ReinforceAgent(\n",
    "#     train_env.time_step_spec(),\n",
    "#     train_env.action_spec(),\n",
    "#     actor_network=actor_net,\n",
    "#     optimizer=optimizer,\n",
    "#     train_step_counter=train_step_counter)\n",
    "\n",
    "# returns, video_filename = train_and_evaluate_agent(tf_agent, name='reinforce')\n",
    "# embed_mp4(video_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO\n",
    "\n",
    "Proximal Policy Optimization: Actor-Critic. Maximizes a surrogate objective function which gives a conservative esimate for how much $\\rho(\\theta)$ will change as a result of the update. This actor critic scheme which uses bounded updates to the policy in order to make the learning process very stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from tf_agents.agents.ppo import ppo_agent\n",
    "\n",
    "# from tf_agents.networks.value_network import ValueNetwork\n",
    "\n",
    "# actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "#     train_env.observation_spec(),\n",
    "#     train_env.action_spec(),\n",
    "#     fc_layer_params=fc_layer_params)\n",
    "\n",
    "# value_net = ValueNetwork(\n",
    "#     train_env.observation_spec()\n",
    "# )\n",
    "\n",
    "# optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "# tf_agent = ppo_agent.PPOAgent(\n",
    "#     train_env.time_step_spec(),\n",
    "#     train_env.action_spec(),\n",
    "#     actor_net=actor_net,\n",
    "#     value_net=value_net,\n",
    "#     optimizer=optimizer,\n",
    "#     train_step_counter=train_step_counter)\n",
    "\n",
    "# returns, video_filename = train_and_evaluate_agent(tf_agent, name='ppo')\n",
    "# embed_mp4(video_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ddpg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One of the Tensors in `experience` has a time axis dim value '402', but we require dim value '2'. Full shape structure of experience:\nTrajectory(step_type=TensorShape([1, 402]), observation=TensorShape([1, 402, 3]), action=TensorShape([1, 402, 1]), policy_info=(), next_step_type=TensorShape([1, 402]), reward=TensorShape([1, 402]), discount=TensorShape([1, 402]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9a0d39d79278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     train_step_counter=train_step_counter)\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ddpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0membed_mp4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-961d251209bf>\u001b[0m in \u001b[0;36mtrain_and_evaluate_agent\u001b[0;34m(tf_agent, name)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Use data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mexperience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tf_agents/agents/tf_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, experience, weights)\u001b[0m\n\u001b[1;32m    215\u001b[0m           \"experience must be type Trajectory, saw type: %s\" % type(experience))\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_trajectory_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tf_agents/agents/tf_agent.py\u001b[0m in \u001b[0;36m_check_trajectory_dimensions\u001b[0;34m(self, experience)\u001b[0m\n\u001b[1;32m    174\u001b[0m               (t.shape[1], self.train_sequence_length, debug_str))\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tf_agents/agents/tf_agent.py\u001b[0m in \u001b[0;36mcheck_shape\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    172\u001b[0m               \u001b[0;34m\"'%s', but we require dim value '%d'. Full shape structure of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m               \u001b[0;34m\"experience:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m               (t.shape[1], self.train_sequence_length, debug_str))\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: One of the Tensors in `experience` has a time axis dim value '402', but we require dim value '2'. Full shape structure of experience:\nTrajectory(step_type=TensorShape([1, 402]), observation=TensorShape([1, 402, 3]), action=TensorShape([1, 402, 1]), policy_info=(), next_step_type=TensorShape([1, 402]), reward=TensorShape([1, 402]), discount=TensorShape([1, 402]))"
     ]
    }
   ],
   "source": [
    "from tf_agents.agents.ddpg import ddpg_agent\n",
    "from tf_agents.agents.ddpg.critic_network import CriticNetwork\n",
    "\n",
    "critic_joint_fc_layer_params = (100,)\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=(100,100))\n",
    "\n",
    "critic_net = CriticNetwork(\n",
    "    (train_env.observation_spec(), train_env.action_spec()),\n",
    "    joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    ")\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = ddpg_agent.DdpgAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_net,\n",
    "    critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=learning_rate),\n",
    "    td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "returns, video_filename = train_and_evaluate_agent(tf_agent, name='ddpg')\n",
    "embed_mp4(video_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The two structures don't have the same nested structure.\n\nFirst structure: type=EagerTensor str=tf.Tensor([[-0.5044885  0.8634184 -0.2572817]], shape=(1, 3), dtype=float32)\n\nSecond structure: type=tuple str=(BoundedTensorSpec(shape=(3,), dtype=tf.float32, name='observation', minimum=array([-1., -1., -8.], dtype=float32), maximum=array([1., 1., 8.], dtype=float32)), BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='action', minimum=array(-2., dtype=float32), maximum=array(2., dtype=float32)))\n\nMore specifically: Substructure \"type=tuple str=(BoundedTensorSpec(shape=(3,), dtype=tf.float32, name='observation', minimum=array([-1., -1., -8.], dtype=float32), maximum=array([1., 1., 8.], dtype=float32)), BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='action', minimum=array(-2., dtype=float32), maximum=array(2., dtype=float32)))\" is a sequence, while substructure \"type=EagerTensor str=tf.Tensor([[-0.5044885  0.8634184 -0.2572817]], shape=(1, 3), dtype=float32)\" is not\nEntire first structure:\n.\nEntire second structure:\n(., .)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[0;34m(nest1, nest2, check_types, expand_composites)\u001b[0m\n\u001b[1;32m    377\u001b[0m     _pywrap_utils.AssertSameStructure(nest1, nest2, check_types,\n\u001b[0;32m--> 378\u001b[0;31m                                       expand_composites)\n\u001b[0m\u001b[1;32m    379\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=EagerTensor str=tf.Tensor([[-0.5044885  0.8634184 -0.2572817]], shape=(1, 3), dtype=float32)\n\nSecond structure: type=tuple str=(BoundedTensorSpec(shape=(3,), dtype=tf.float32, name='observation', minimum=array([-1., -1., -8.], dtype=float32), maximum=array([1., 1., 8.], dtype=float32)), BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='action', minimum=array(-2., dtype=float32), maximum=array(2., dtype=float32)))\n\nMore specifically: Substructure \"type=tuple str=(BoundedTensorSpec(shape=(3,), dtype=tf.float32, name='observation', minimum=array([-1., -1., -8.], dtype=float32), maximum=array([1., 1., 8.], dtype=float32)), BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='action', minimum=array(-2., dtype=float32), maximum=array(2., dtype=float32)))\" is a sequence, while substructure \"type=EagerTensor str=tf.Tensor([[-0.5044885  0.8634184 -0.2572817]], shape=(1, 3), dtype=float32)\" is not",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4995bc8677ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     train_step_counter=train_step_counter)\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sac'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0membed_mp4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-961d251209bf>\u001b[0m in \u001b[0;36mtrain_and_evaluate_agent\u001b[0;34m(tf_agent, name)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Evaluate the agent's policy once before training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     avg_return = compute_avg_return(\n\u001b[0;32m--> 108\u001b[0;31m         eval_env, tf_agent.policy, num_eval_episodes)\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-961d251209bf>\u001b[0m in \u001b[0;36mcompute_avg_return\u001b[0;34m(environment, policy, num_episodes)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \"\"\"\n\u001b[1;32m    468\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ppo_policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tf_agents/policies/actor_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# Actor network outputs nested structure of distributions or actions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     actions_or_distributions, policy_state = self._apply_actor_network(\n\u001b[0;32m--> 149\u001b[0;31m         network_observation, time_step.step_type, policy_state, mask=mask)\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_to_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_or_distribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tf_agents/policies/actor_policy.py\u001b[0m in \u001b[0;36m_apply_actor_network\u001b[0;34m(self, observation, step_type, policy_state, mask)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m       return self._actor_network(\n\u001b[0;32m--> 124\u001b[0;31m           observation, step_type, policy_state, training=self._training)\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m       return self._actor_network(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_network_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \"\"\"\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_tensor_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0mnetwork_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"network_state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnetwork_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36massert_same_structure\u001b[0;34m(nest1, nest2, check_types, expand_composites)\u001b[0m\n\u001b[1;32m    383\u001b[0m                   \u001b[0;34m\"Entire first structure:\\n%s\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                   \u001b[0;34m\"Entire second structure:\\n%s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                   % (str(e), str1, str2))\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=EagerTensor str=tf.Tensor([[-0.5044885  0.8634184 -0.2572817]], shape=(1, 3), dtype=float32)\n\nSecond structure: type=tuple str=(BoundedTensorSpec(shape=(3,), dtype=tf.float32, name='observation', minimum=array([-1., -1., -8.], dtype=float32), maximum=array([1., 1., 8.], dtype=float32)), BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='action', minimum=array(-2., dtype=float32), maximum=array(2., dtype=float32)))\n\nMore specifically: Substructure \"type=tuple str=(BoundedTensorSpec(shape=(3,), dtype=tf.float32, name='observation', minimum=array([-1., -1., -8.], dtype=float32), maximum=array([1., 1., 8.], dtype=float32)), BoundedTensorSpec(shape=(1,), dtype=tf.float32, name='action', minimum=array(-2., dtype=float32), maximum=array(2., dtype=float32)))\" is a sequence, while substructure \"type=EagerTensor str=tf.Tensor([[-0.5044885  0.8634184 -0.2572817]], shape=(1, 3), dtype=float32)\" is not\nEntire first structure:\n.\nEntire second structure:\n(., .)"
     ]
    }
   ],
   "source": [
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.agents.ddpg.critic_network import CriticNetwork\n",
    "from tf_agents.networks import normal_projection_network\n",
    "\n",
    "critic_joint_fc_layer_params = (100,)\n",
    "\n",
    "\n",
    "def normal_projection_net(action_spec, init_means_output_factor=0.1):\n",
    "    return normal_projection_network.NormalProjectionNetwork(\n",
    "        action_spec,\n",
    "        mean_transform=None,\n",
    "        state_dependent_std=True,\n",
    "        init_means_output_factor=init_means_output_factor,\n",
    "        std_transform=sac_agent.std_clip_transform,\n",
    "        scale_distribution=True)\n",
    "\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params,\n",
    "    continuous_projection_net=normal_projection_net)\n",
    "\n",
    "critic_net = CriticNetwork(\n",
    "    (train_env.observation_spec(), train_env.action_spec()),\n",
    "    joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    ")\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "tf_agent = sac_agent.SacAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_net,\n",
    "    critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=learning_rate),\n",
    "    alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=learning_rate),\n",
    "    td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "returns, video_filename = train_and_evaluate_agent(tf_agent, name='sac')\n",
    "embed_mp4(video_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
